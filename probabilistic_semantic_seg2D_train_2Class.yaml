mode: train

data:
  category: "pair"
  data_path: {"train":"./data/new_train_2class_v7/", "val":"./data/new_train_2class_val/"}
  dataloader:
    train:
      dataloader_params:
        batch_size: 2
        pin_memory: True
        num_workers: 16
    val:
      dataloader_params:
        batch_size: 1
        pin_memory: True
        num_workers: 8

  preprocess:
    - module_name: monai.transforms
      func_name: LoadImaged
      params:
        keys: ["IM"]
        dimension_order_out: "CYX"
        T: 0
        Z: 0
    - module_name: monai.transforms
      func_name: LoadImaged
      params:
        keys: ["GT"]
        dtype: int
        dimension_order_out: "YX"
        C: 0
        T: 0
        Z: 0
    - module_name: monai.transforms
      func_name: EnsureChannelFirstd  
      params:
        keys: ["GT"]
        channel_dim: "no_channel" 
    - module_name: monai.transforms
      func_name: NormalizeIntensityd
      params:
        channel_wise: True
        keys: ["IM"]
    - module_name: monai.transforms
      func_name: RandSpatialCropSamplesd
      params:
        keys: ["IM", "GT"]
        random_size: False
        num_samples: 1
        roi_size: [512, 512]
    - module_name: monai.transforms
      func_name: EnsureTyped
      params:
        keys: ["IM", "GT"]

  augmentation:
    - module_name: monai.transforms
      func_name: RandFlipd
      params:
        prob: 0.5
        keys: ["IM", "GT"]
    - module_name: monai.transforms
      func_name: RandRotate90d
      params:
        prob: 0.5
        keys: ["IM", "GT"]
    - module_name: monai.transforms
      func_name: RandHistogramShiftd
      params:
        prob: 0.2
        num_control_points: 50
        keys: ["IM"]
    - module_name: monai.transforms
      func_name: Rand2DElasticd
      params:
        prob: 0.2
        spacing: [32, 32]
        magnitude_range: [1, 5]
        rotate_range: [0, 0.5]
        scale_range: [0.1, 0.25]
        translate_range: [10, 50]
        padding_mode: "reflection"
        mode: "nearest"
        keys: ["IM", "GT"]

model:
  framework: ProbUnet
  net:
    module_name: mmv_im2im.models.nets.ProbUnet
    func_name: ProbabilisticUNet
    params:
      in_channels: 2
      n_classes: 3 # 0 background + 2 classes
      latent_dim: 6
      kl_clamp: 20.0 # For kl stability and convergence
      #elbo_class_weights: [1.0, 1.0, 1.9] #class unbalance managment
      use_fractal_regularization: False # Set to True to enable fractal regularization and then the parametert for this
      fractal_weight: 0.4  # loss fractal weight.
      fractal_num_kernels: 10 # Number of kernel used.
      fractal_mode: "classic" # aproximation type for the FD ("classic" or "entropy").
      fractal_to_binary: True #binarization for the masks (classes doent matter just the geometry)
      use_connectivity_regularization : False
      connectivity_weight : 0.53
      connectivity_kernel_size : 9 #  3, 5, 7
      connectivity_ignore_background : True
      use_gdl_focal_regularization: False
      gdl_focal_weight: 0.5
      #gdl_class_weights: [1.0,1.0,2]
      use_topological_regularization: False # Enable top features
      topological_weight: 0.001 # Adjust this weight based on the importance of this regularization.
      topological_dim: 2 # 2 for 2D images, 3 for 3D.
      topological_connectivity: 8 # 4 or 8 for 2D; 6 or 26 for 3D.
      topological_inclusion: [] 
      topological_exclusion: [] 
      topological_min_thick: 1 # Default value.

  criterion:
    module_name: mmv_im2im.utils.elbo_loss 
    func_name: ELBOLoss 
    params:
      beta: 0.5 
      n_classes: 3 

  optimizer:
    module_name: torch.optim
    func_name: AdamW
    params:
      lr: 0.001 
      weight_decay: 0.0001

  scheduler:
    module_name: torch.optim.lr_scheduler
    func_name: ReduceLROnPlateau
    params:
      mode: 'min'
      factor: 0.5
      patience: 10
    monitor: 'val_loss'

trainer:
  verbose: True
  params:
    precision: 32 
    max_epochs: 3000
    detect_anomaly: True
  callbacks:
    - module_name: lightning.pytorch.callbacks.early_stopping
      func_name: EarlyStopping
      params:
        monitor: 'val_loss'
        patience: 80
        verbose: True
    - module_name: lightning.pytorch.callbacks.model_checkpoint
      func_name: ModelCheckpoint
      params:
        monitor: 'val_loss'
        filename: '{epoch}-{val_loss:.5f}'
        mode: min
        save_top_k: 5
        save_last: true
