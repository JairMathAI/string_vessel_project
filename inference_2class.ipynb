{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from aicsimageio import AICSImage\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "from mmv_im2im.configs.config_base import ProgramConfig, configuration_validation\n",
    "from mmv_im2im import ProjectTester\n",
    "from skimage.morphology import remove_small_objects, remove_small_holes\n",
    "from skimage.measure import label, regionprops\n",
    "from aicssegmentation.core.utils import topology_preserving_thinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need to re-define a utility function for jupyter notebook. No need to change anything. Just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from pyrallis import field\n",
    "\n",
    "import argparse\n",
    "import dataclasses\n",
    "import sys\n",
    "import warnings\n",
    "from argparse import HelpFormatter, Namespace\n",
    "from collections import defaultdict\n",
    "from logging import getLogger\n",
    "from typing import Dict, List, Sequence, Text, Type, Union, TypeVar, Generic, Optional\n",
    "\n",
    "from pyrallis import utils, cfgparsing\n",
    "from pyrallis.help_formatter import SimpleHelpFormatter\n",
    "from pyrallis.parsers import decoding\n",
    "from pyrallis.utils import Dataclass, PyrallisException\n",
    "from pyrallis.wrappers import DataclassWrapper\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class ArgumentParser(Generic[T], argparse.ArgumentParser):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_class: Type[T],\n",
    "        config: Optional[str] = None,\n",
    "        formatter_class: Type[HelpFormatter] = SimpleHelpFormatter,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Creates an ArgumentParser instance.\"\"\"\n",
    "        kwargs[\"formatter_class\"] = formatter_class\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # constructor arguments for the dataclass instances.\n",
    "        # (a Dict[dest, [attribute, value]])\n",
    "        self.constructor_arguments: Dict[str, Dict] = defaultdict(dict)\n",
    "\n",
    "        self._wrappers: List[DataclassWrapper] = []\n",
    "\n",
    "        self.config = config\n",
    "        self.config_class = config_class\n",
    "\n",
    "        self._assert_no_conflicts()\n",
    "        self.add_argument(\n",
    "            f\"--{utils.CONFIG_ARG}\",\n",
    "            type=str,\n",
    "            help=\"Path for a config file to parse with pyrallis\",\n",
    "        )\n",
    "        self.set_dataclass(config_class)\n",
    "\n",
    "    def set_dataclass(\n",
    "        self,\n",
    "        dataclass: Union[Type[Dataclass], Dataclass],\n",
    "        prefix: str = \"\",\n",
    "        default: Union[Dataclass, Dict] = None,\n",
    "        dataclass_wrapper_class: Type[DataclassWrapper] = DataclassWrapper,\n",
    "    ):\n",
    "        \"\"\"Adds command-line arguments for the fields of `dataclass`.\"\"\"\n",
    "        if not isinstance(dataclass, type):\n",
    "            default = dataclass if default is None else default\n",
    "            dataclass = type(dataclass)\n",
    "\n",
    "        new_wrapper = dataclass_wrapper_class(dataclass, prefix=prefix, default=default)\n",
    "        self._wrappers.append(new_wrapper)\n",
    "        self._wrappers += new_wrapper.descendants\n",
    "\n",
    "        for wrapper in self._wrappers:\n",
    "            logger.debug(\n",
    "                f\"Adding arguments for dataclass: {wrapper.dataclass} \"\n",
    "                f\"at destination {wrapper.dest}\"\n",
    "            )\n",
    "            wrapper.add_arguments(parser=self)\n",
    "\n",
    "    def _assert_no_conflicts(self):\n",
    "        \"\"\"Checks for a field name that conflicts with utils.CONFIG_ARG\"\"\"\n",
    "        if utils.CONFIG_ARG in [\n",
    "            field.name for field in dataclasses.fields(self.config_class)\n",
    "        ]:\n",
    "            raise PyrallisException(\n",
    "                f\"{utils.CONFIG_ARG} is a reserved word for pyrallis\"\n",
    "            )\n",
    "\n",
    "    def parse_args(self, args=None, namespace=None) -> T:\n",
    "        return super().parse_args(args, namespace)\n",
    "\n",
    "    def parse_known_args(\n",
    "        self,\n",
    "        args: Sequence[Text] = None,\n",
    "        namespace: Namespace = None,\n",
    "        attempt_to_reorder: bool = False,\n",
    "    ):\n",
    "        # NOTE: since the usual ArgumentParser.parse_args() calls\n",
    "        # parse_known_args, we therefore just need to overload the\n",
    "        # parse_known_args method to support both.\n",
    "        if args is None:\n",
    "            # args default to the system args\n",
    "            args = sys.argv[1:]\n",
    "        else:\n",
    "            # make sure that args are mutable\n",
    "            args = list(args)\n",
    "\n",
    "        if \"--help\" not in args:\n",
    "            for action in self._actions:\n",
    "                # TODO: Find a better way to do that?\n",
    "                action.default = (\n",
    "                    argparse.SUPPRESS\n",
    "                )  # To avoid setting of defaults in actual run\n",
    "                action.type = (\n",
    "                    str  # In practice, we want all processing to happen with yaml\n",
    "                )\n",
    "        parsed_args, unparsed_args = super().parse_known_args(args, namespace)\n",
    "\n",
    "        parsed_args = self._postprocessing(parsed_args)\n",
    "        return parsed_args, unparsed_args\n",
    "\n",
    "    def print_help(self, file=None):\n",
    "        return super().print_help(file)\n",
    "\n",
    "    def _postprocessing(self, parsed_args: Namespace) -> T:\n",
    "        logger.debug(\"\\nPOST PROCESSING\\n\")\n",
    "        logger.debug(f\"(raw) parsed args: {parsed_args}\")\n",
    "\n",
    "        parsed_arg_values = vars(parsed_args)\n",
    "\n",
    "        for key in parsed_arg_values:\n",
    "            parsed_arg_values[key] = cfgparsing.parse_string(parsed_arg_values[key])\n",
    "\n",
    "        config = self.config  # Could be NONE\n",
    "\n",
    "        if utils.CONFIG_ARG in parsed_arg_values:\n",
    "            new_config = parsed_arg_values[utils.CONFIG_ARG]\n",
    "            if config is not None:\n",
    "                warnings.warn(\n",
    "                    UserWarning(f\"Overriding default {config} with {new_config}\")\n",
    "                )\n",
    "            ######################################################################\n",
    "            # adapted from original implementation in pyrallis\n",
    "            ######################################################################\n",
    "            if Path(new_config).is_file():\n",
    "                # pass in a absolute path\n",
    "                config = new_config\n",
    "            else:\n",
    "                new_config = str(new_config)\n",
    "                print(f\"trying to locate preset config for {new_config} ...\")\n",
    "\n",
    "                config = Path(__file__).parent / f\"preset_{new_config}.yaml\"\n",
    "            del parsed_arg_values[utils.CONFIG_ARG]\n",
    "\n",
    "        if config is not None:\n",
    "            print(f\"loading configuration from {config} ...\")\n",
    "            file_args = cfgparsing.load_config(open(config, \"r\"))\n",
    "            file_args = utils.flatten(file_args, sep=\".\")\n",
    "            file_args.update(parsed_arg_values)\n",
    "            parsed_arg_values = file_args\n",
    "            print(\"configuration loading is completed\")\n",
    "\n",
    "        deflat_d = utils.deflatten(parsed_arg_values, sep=\".\")\n",
    "        cfg = decoding.decode(self.config_class, deflat_d)\n",
    "\n",
    "        return cfg\n",
    "\n",
    "def parse_adaptor_jpnb(\n",
    "    config_class: Type[T],\n",
    "    config: Optional[Union[Path, str]] = None,\n",
    "    args: Optional[Sequence[str]] = None,\n",
    ") -> T:\n",
    "    parser = ArgumentParser(config_class=config_class, config=config)\n",
    "    return parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start to do some real processing.\n",
    "\n",
    "<b>Note:</b>\n",
    "<br>\n",
    "<br>\n",
    "Some changes are required depending on the model you intend to use for prediction:\n",
    "\n",
    "To change the model's configuration, simply replace the existing config parameter with the desired one in the line of the code below:\n",
    "\n",
    "```python \n",
    "   cfg = parse_adaptor_jpnb(config_class=ProgramConfig, config=\"./semantic_seg_2d_inference_2class.yaml\")\n",
    "```\n",
    "\n",
    " For the retarining of the existing model with improvements no changes are needed here:\n",
    "\n",
    "```python\n",
    "   cfg = parse_adaptor_jpnb(config_class=ProgramConfig, config=\"./semantic_seg_2d_inference_2class.yaml\") \n",
    "```\n",
    "   \n",
    " For the Probabilistic model use:  \n",
    " \n",
    "```python\n",
    "   cfg = parse_adaptor_jpnb(config_class=ProgramConfig, config=\"./probabilistic_semantic_seg2D_inference_2Class.yaml\")  \n",
    "```\n",
    " \n",
    "Depending on your model selection, you'll need to choose the correct weights.<br>\n",
    "This means changing the input in the following line of the code below:\n",
    "\n",
    "```python \n",
    "   cfg.model.checkpoint = Path(\"./version_2023_06.ckpt\")\n",
    "```\n",
    "\n",
    "If you desire to test the retraining of the last model, you need to use: \n",
    "\n",
    "```python \n",
    "   cfg.model.checkpoint = Path(\"./ClassicUnet_16_06_2025.ckpt\")\n",
    "```\n",
    "\n",
    "If you desire to test the probabilistic model you need to use:\n",
    "\n",
    "```python \n",
    "   cfg.model.checkpoint = Path(\"./ProbUnet_16_06_2025.ckpt\")\n",
    "```\n",
    "\n",
    "These .ckpt files are now available via the same download [LINK](https://ambiomcloud.isas.de/index.php/s/CwcfFRt8eQ9gKWj)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = parse_adaptor_jpnb(config_class=ProgramConfig, config=\"./semantic_seg_2d_inference_2class.yaml\")\n",
    "cfg = configuration_validation(cfg)\n",
    "\n",
    "# select which model to use\n",
    "# the old one (which used for genrating the results evaluated by Jan): version_2023_06.ckpt\n",
    "# the recent one (slightly improved for antigen data): version_2023_09.ckpt\n",
    "cfg.model.checkpoint = Path(\"./version_2023_06.ckpt\")\n",
    "\n",
    "# define the executor for inference (no need to change anything)\n",
    "executor = ProjectTester(cfg)\n",
    "executor.setup_model()\n",
    "executor.setup_data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data, run inference, and save the result (set your path)\n",
    "\n",
    "# root path, should be the same as \"out_path_base\" in the data wrangling notebook\n",
    "path_base = Path(\"/path/to/splitted/3d/files\")\n",
    "\n",
    "# input 3D tiff files. Note. we assume data were generated by the data_wrangling notebook\n",
    "# the tiff file has two channels, first is CD31, second is Coll IV\n",
    "input_path = path_base / Path(\"split_3d\")\n",
    "out_p = path_base / Path(\"pred_2class\")\n",
    "out_p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# get all files to be processed\n",
    "filenames = sorted(input_path .glob(\"*.tiff\"))\n",
    "\n",
    "num = 0\n",
    "for fn in filenames:\n",
    "    num = num + 1\n",
    "    print(\"--Predicting: \", num,'/',len(filenames), \"...\")\n",
    "    img = AICSImage(fn).get_image_data(\"CZYX\", T=0)\n",
    "\n",
    "    out_list = []\n",
    "    for zz in range(img.shape[1]):\n",
    "        # channel order: CD31, Coll IV\n",
    "        im_input = img[:, zz, :, :]\n",
    "        seg = executor.process_one_image(im_input)\n",
    "        out_list.append(np.squeeze(seg))\n",
    "    seg_full = np.stack(out_list, axis=0)\n",
    "\n",
    "    ###################################################\n",
    "    # attempt to remove pericytes by post-processing\n",
    "    ###################################################\n",
    "    # remove small objects (size<64)\n",
    "    seg_2 = remove_small_objects(seg_full == 2, min_size=64)\n",
    "\n",
    "    # for all mid-size objects (64<s<300), check slice by slice for circles\n",
    "    seg_2_mid = np.logical_xor(seg_2, remove_small_objects(seg_2, min_size=300))\n",
    "    for zz in range(seg_2_mid.shape[0]):\n",
    "        seg_label, num_obj = label(seg_2_mid[zz, :, :], return_num=True)\n",
    "        if num_obj > 0:\n",
    "            stats = regionprops(seg_label)\n",
    "            for ii in range(num_obj):\n",
    "                # low eccentricity (closer to circle) and not too concave\n",
    "                if stats[ii].eccentricity < 0.88 and stats[ii].solidity > 0.85 and stats[ii].area < 150:\n",
    "                    seg_z = seg_2[zz, :, :]\n",
    "                    seg_z[seg_label == (ii+1)] = 0\n",
    "                    seg_2[zz, :, :] = seg_z\n",
    "\n",
    "    seg_full[seg_full == 2] = 1\n",
    "    seg_full[seg_2 > 0] = 2\n",
    "\n",
    "    ###################################################################\n",
    "    # another minor fix: remove small holes due to segmentation errors\n",
    "    ###################################################################\n",
    "    hole_size_threshold = 15\n",
    "    seg_1 = remove_small_objects(seg_full==1, min_size=50)\n",
    "    seg_2 = seg_full == 2\n",
    "    for zz in range(seg_full.shape[0]):\n",
    "        s_v = remove_small_holes(seg_1[zz, :, :], area_threshold=hole_size_threshold)\n",
    "        seg_1[zz, :, :] = s_v[:, :]\n",
    "\n",
    "        a_v = remove_small_holes(seg_2[zz, :, :], area_threshold=hole_size_threshold)\n",
    "        seg_2[zz, :, :] = a_v[:, :]\n",
    "\n",
    "    ##################################################\n",
    "    # thickness adjustment\n",
    "    ##################################################\n",
    "    # perform thinning on segmentation without breaking topology\n",
    "    # this is only done for string vessels. Currently, we only removing 1 pixel\n",
    "    # from the outer laye of the segmented string vessels. If this is not enough,\n",
    "    # one can set the paramter thin=1 to thin=2 or higher.\n",
    "    seg_string = topology_preserving_thinning(seg_full == 2, min_thickness=1, thin=1)\n",
    "    seg_thin = np.zeros_like(seg_full)\n",
    "    seg_thin[seg_string > 0] = 2\n",
    "    seg_thin[seg_full == 1] = 1  # the normal vessels are unchanged\n",
    "\n",
    "    out_fn = out_p / fn.name\n",
    "    OmeTiffWriter.save(seg_full, out_fn, dim_order=\"ZYX\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc_workbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0a6b497edbe280d9c9c3fe7b26d28e91277df791e6ef10dec6644aa4f1f4e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
